FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      python3 python3-venv python3-pip python3-dev \
      git ca-certificates curl \
      build-essential gcc g++ && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Create venv for isolation
RUN python3 -m venv /app/.venv
ENV PATH=/app/.venv/bin:$PATH
ENV CC=gcc

# Pin library versions for stability; torch with CUDA must match base image
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu121 torch==2.4.1 && \
    pip install --no-cache-dir \
      fastapi==0.115.0 \
      uvicorn[standard]==0.30.6 \
      transformers \
      accelerate \
      triton==3.4.0 \
      hf_transfer \
      kernels \
      sentencepiece \
      einops

# PyTorch is installed with CUDA 12.1 wheels. Run the container with `--gpus all`.

# App files
COPY gpt /app/gpt

# Sensible defaults for HF caches and runtime behavior; can be overridden at runtime
ENV RAWORC_GPT_MODEL="openai/gpt-oss-120b"
ENV HOST=0.0.0.0
ENV PORT=6000
ENV HF_HOME=/app/data/hf
ENV HUGGINGFACE_HUB_CACHE=/app/data/hf
ENV HF_HUB_DISABLE_TELEMETRY=1
ENV HF_HUB_ENABLE_HF_TRANSFER=1
ENV TOKENIZERS_PARALLELISM=false
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
ENV PYTHONUNBUFFERED=1

# Ensure default data/logs paths exist (volumes may override at runtime)
RUN mkdir -p /app/data/hf /app/logs

EXPOSE 6000

# Healthcheck hits /ready and expects status=ready
HEALTHCHECK --interval=15s --timeout=5s --start-period=20s --retries=10 \
  CMD curl -fsS http://127.0.0.1:6000/ready | grep -q '"status":"ready"' || exit 1

CMD ["/app/.venv/bin/uvicorn", "gpt.app:app", "--host", "0.0.0.0", "--port", "6000"]
